{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ed0af0e",
   "metadata": {},
   "source": [
    "## CS6370: Natural Language Processing\n",
    "### Assignment 1\n",
    "\n",
    "#### Team Members:\n",
    "Rishaab Karthik - MM19B046\n",
    "\n",
    "Aditi - MM19B020\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8142463a",
   "metadata": {},
   "source": [
    "#### 1. What is the simplest and obvious top-down approach to sentence segmentation for English texts?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3ff437",
   "metadata": {},
   "source": [
    "We know that sentences start with Capital letters and they are preceded with sentences ending with full stops. This is the top down approach which we can use for sentence segmentation in English texts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97c5360",
   "metadata": {},
   "source": [
    "#### 2.  Does the top-down approach (your answer to the above question) always do correct sentence segmentation? If Yes, justify. If No, substantiate it with a counter example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7336f13",
   "metadata": {},
   "source": [
    "No sometimes there are acronyms ending with a full stop which might be followed by a capital letter as well in the middle of a sentence which will be a wrong segmentation. Example: Contact Mahindra Pvt. Ltd. Or there are cases where there are typos and  people do not following proper sentence structure like punctuating sentences or capitalize first letters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca477b2",
   "metadata": {},
   "source": [
    "#### 3. Python NLTK is one of the most commonly used packages for Natural Language Processing. What does the Punkt Sentence Tokenizer in NLTK do differently from the simple top-down approach? You can read about the tokenizer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fff728",
   "metadata": {},
   "source": [
    "This tokenizer divides a text into a list of sentences by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences. It must be trained on a large collection of plaintext in the target language before it can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2af6e54",
   "metadata": {},
   "source": [
    "**We next import the necessary libraries required.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8771995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4eec45dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 2,\n",
       " 'author': 'ting-yili',\n",
       " 'bibliography': 'department of aeronautical engineering, rensselaer polytechnic institute troy, n.y.',\n",
       " 'body': \"simple shear flow past a flat plate in an incompressible fluid of small viscosity . in the study of high-speed viscous flow past a two-dimensional body it is usually necessary to consider a curved shock wave emitting from the nose or leading edge of the body .  consequently, there exists an inviscid rotational flow region between the shock wave and the boundary layer .  such a situation arises, for instance, in the study of the hypersonic viscous flow past a flat plate .  the situation is somewhat different from prandtl's classical boundary-layer problem . in prandtl's original problem the inviscid free stream outside the boundary layer is irrotational while in a hypersonic boundary-layer problem the inviscid free stream must be considered as rotational .  the possible effects of vorticity have been recently discussed by ferri and libby .  in the present paper, the simple shear flow past a flat plate in a fluid of small viscosity is investigated .  it can be shown that this problem can again be treated by the boundary-layer approximation, the only novel feature being that the free stream has a constant vorticity .  the discussion here is restricted to two-dimensional incompressible steady flow .\",\n",
       " 'title': 'simple shear flow past a flat plate in an incompressible fluid of small viscosity .'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open('Cranfield Dataset NLP/cranfield/cran_docs.json')\n",
    "data=json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "699c7e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topdown(s):    \n",
    "    l=[]\n",
    "    j=0\n",
    "    i=0\n",
    "    while(i<len(s)):\n",
    "        if s[i].isupper():\n",
    "            if i==0:\n",
    "                j=s.find('.')\n",
    "            elif(s[i-1]=='.' or s[i-2]=='.'):\n",
    "                j=i+s[i:].find('.')\n",
    "        if j>i:\n",
    "            l.append(s[i:j]+'.')\n",
    "            i=j+2\n",
    "        else:\n",
    "            i+=1\n",
    "\n",
    "    return (l)\n",
    "text = [['Hi', 'this', 'is', 'a', 'test', 'piece', 'of', 'text','.'], ['I', 'am', 'trying', 'to', 'understand', 'if', 'this', 'works,I', 'think', 'it', 'will','.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1b486be",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordRemovedText = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "for sentence in text:\n",
    "    temp = [word for word in sentence if word not in stop_words]\n",
    "    stopwordRemovedText.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fb0470c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aditib/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7f6af96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hi', 'test', 'piece', 'text', '.'],\n",
       " ['I', 'trying', 'understand', 'works,I', 'think', '.']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwordRemovedText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570d304b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
